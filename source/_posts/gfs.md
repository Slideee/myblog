---
title: Google File System（GFS）
date: 2022/10/16
updated: 2022/10/23
tag: Paper
author: Slide
categories: Paper
---

最近在刷MIT6.824，顺便读完了早年谷歌分布式领域非常经典的三大论文之一GFS，发现论文中很多简洁和经典的设计依旧能在20年后的系统中看到影子。GFS文件系统展示了一个使用普通硬件支持大规模数据处理的系统的特质。本文会花一到两周的时间对整篇论文以及工作中遇到的一些问题进行总结和思考。

<!--more-->

# 1 简介

GFS与传统的分布式文件系统有着很多相同的设计目标。包括性能、可伸缩性、可靠性以及可用性。同时GFS和早期文件系统的假设都有明显的不同，从而衍生出了完全不同的设计思路。

1. 首先组件失效被认为是常态事件，而不是意外事件。这意味着，持续的监控告警，灾难冗余和自动恢复等机制就必须集成在GFS中。
2. 其次，文件非常巨大，I/O操作和块的尺寸都需要重新考虑。
3. 绝大部分的文件的修改是采用在文件尾部追加数据，而不是覆盖原有数据的方式。
4. 应用程序和文件系统API的协同设计提高了整个系统的灵活性。

# 2 设计概述

## 2.1 设计预期

1. 系统必须持续监控自身的状态，它必须将组件失效作为一种常态，能够迅速地侦测、冗余并恢复失效的组件。
2. 系统的工作负载主要由两种读操作组成：**大规模的流式读取和小规模的随机读取**。
3. 系统的工作负载还包括许多大规模的、顺序的、数据追加方式的写操作。
4. 系统必须高效的、行为定义明确的2实现多客户端并行追加数据到同一个文件里的语意。
5. 高性能的稳定网络带宽远比低延迟重要。

## 2.2 接口

GFS 提供了一套类似传统文件系统的 API 接口函数，虽然并不是严格按照 POSIX 等标准 API 的形式实现的。文件以分层目录的形式组织，用路径名来标识。我们支持常用的操作，如创建新文件、删除文件、打开文件、关闭文件、读和写文件。

## 2.3 架构

![gfs-arch](/images/posts/gfs/gfs-arch.png)

一个 GFS 集群包含一个单独的 Master 节点3、多台 Chunk 服务器，并且同时被多个客户端访问。所有的这些机器通常都是普通的 Linux 机器，运行着用户级别(user-level)的服务进程。

**GFS 存储的文件都被分割成固定大小的 Chunk。**

1. 在 Chunk 创建的时候，Master 服务器会给每个 Chunk 分配一个不变的、全球唯一的 64 位的 Chunk 标识。
2. Chunk 服务器把 Chunk 以 Linux 文件的形式保存在本地硬盘上，并且根据指定的 Chunk 标识和字节范围来读写块数据。每个块都会复制到多个块服务器上。
3. 缺省情况下，使用 3 个存储复制节点，不过用户可以为不同的文件命名空间设定不同的复制级别。

**Master 节点管理所有的文件系统元数据。**

1. 这些元数据包括名字空间、访问控制信息、文件和 Chunk 的映射信息、以及当前 Chunk 的位置信息。
2. 同时管理着系统范围内的活动，比如，租户，孤儿Chunk回收及Chunk迁移。
3. 最后Master会和每个Chunk Server周期性的心跳通讯。

**GFS 客户端代码以库的形式被链接到客户程序里。**

值得注意的是，客户端和Master节点通信仅仅只获取了元数据，所有的数据操作都是由客户端直接和Chunk Server进行交互的。

**无论是客户端还是 Chunk 服务器都不需要缓存文件数据。**

1. 客户端大部分以流的方式读取一个巨大文件，工作集太大无法被缓存。
2. 服务端Chunk以本地文件的方式保存，Linux操作系统的文件系统缓存会把经常访问的数据缓存在内存中。

## 2.4 单一Master节点

在论文中提到，单一的 Master 节点的策略大大简化了设计。单一的 Master 节点可以通过全局的信息精确定位Chunk 的位置以及进行复制决策。基于此，客户端必须减少对Master节点的读写。同时，客户端把Master作为一个类似Nameserver服务来保持跟自己的通信。

具体通信的流程如图1所示。

1. 客户端把文件名和程序指定的字节偏移，根据固定的 Chunk 大小，转换成文件的Chunk 索引。
2. 客户端把文件名和 Chunk 索引发送给 Master 节点。
3. Master 节点将相应的 Chunk 标识和副本的位置信息发还给客户端。
4. 客户端用文件名和 Chunk 索引作为 key 缓存这些信息。
5. 客户端发送请求到其中的一个副本处，一般会选择最近的。请求信息包含了 Chunk 的标识和字节范围。在对这个 Chunk 的后续读取操作中，客户端不必再和 Master 节点通讯了。

## 2.5 Chunk尺寸

Chunk 的大小是关键的设计参数之一。GFS选择了64MB。每个 Chunk 的副本都以普通 Linux 文件的形式保存在 Chunk 服务器上，只有在需要的时候才扩大。惰性空间分配策略避免了因内部碎片造成的空间浪费，内部碎片或许是对选择这么大的 Chunk 尺寸最具争议一点。

选择较大的 Chunk 尺寸有几个重要的优点。

1. 减少了客户端和Master节点通讯的需求。因为只需要一次和Master节点通信就可以获取Chunk的位置信息，之后就可以对同一个Chunk进行多次的读写操作。
2. 客户端能够对一个块进行多次操作，这样就可以通过与 Chunk 服务器保持较长时间的 TCP 连接来减少网络负载。
3. 减少了 Master节点需要保存的元数据的数量。注意，这里Master的元数据是保存在内存中。

同时，较大尺寸的Chunk也是有缺陷的。

小文件可能包含较少的Chunk或者只有一个Chunk，这样当客户端频繁多次访问这个小文件时，该Chunk的Chunk Server容易变成一个热点。

## 2.6 元数据

Master 服务器存储 3 种主要类型的元数据，**文件和 Chunk 的命名空间、文件和 Chunk 的对应关系、每个 Chunk 副本的存放地点**。所有元数据保存在Master服务器的内存中，前两种类型同时也以记录变更日志的方式记录在OS的系统日志文件中，日志文件存储在本地磁盘，同时会被复制到其他远程Master服务器。

### 2.6.1 Chunk位置信息

Chunk的位置信息本身并不会被Master服务器持久化保存。Master服务器只是在启动的时候轮询Chunk Server来获取这些信息的。这意味着，Master能够保证它持有的Chunk位置信息一直是最新的。

### 2.6.2 操作日志

操作日志包含了关键的元数据变更历史记录。Master 服务器在灾难恢复时，通过重演操作日志把文件系统恢复到最近的状态。具体的，Master会在日志增长到一定量时对系统状态做一次Checkpoint。在灾难恢复时，通过读取这个Checkpoint，并重演Checkpoint之后的有限个日志文件就能够恢复，类似于Redis的RDB和AOF。

## 2.7 一致性模型

GFS 支持一个宽松的一致性模型，该模型很好的支撑了高度分布的应用，同时还保持了相对简单且容易实现的优点。

### 2.7.1 GFS一致性保障机制

首先，文件命名空间的修改（例如，文件创建）是原子性的。它们仅由 Master 节点的控制，命名空间锁提供了原子性和正确性的保障，Master 节点的操作日志定义了这些操作在全局的顺序（2.6.2 章）。

![consistency](/images/posts/gfs/consistency.png)

数据修改后文件region（修改操作所涉及的文件中的某个范围）的状态取决于操作的类型，是否成功，是否同步修改。上图总结了各种操作的结果。

- 所有客户端，无论从哪个副本读取，读到的数据一致，那么我们就认为文件region是"一致的（consistent）"
- 对文件的数据修改之后，region一致，并且客户端能够看到写入操作全部的内容，那么这个region是"已定义（defined）"
- 当一个数据修改操作成功执行，并且没有受到其他并行写操作的干扰，那么影响的region就是已定义（defined）的，所有客户端都可以看到写入的内容。并行修改操作成功完成之后，region处于"一致且未定义（consistent but undefined）"的状态，所有的客户端看到同样的数据，但是无法读到任何一次写入的数据。此时，region内包含了来自多个修改操作数据混杂的数据片段。
- 失败的修改操作导致一个region处于不一致（inconsistent）状态，不同的客户在不同的时间会看到不同的数据。

数据修改操作分为写入或者追加两种。写入操作把数据写在应用程序指定的文件偏移位置上。即使有多个修改操作并行执行时，记录追加操作至少可以把数据原子性的追加到文件中一次，但是偏移位置是由GFS 选择的。

### 2.7.2 程序的实现

在实际应用中，所有的应用程序对文件的写入操作都是尽量采用数据追加方式，而不是覆盖方式。一种典型的应用，应用程序从头到尾写入数据，生成了一个文件。写入所有数据之后，应用程序自动将文件改名为一个永久保存的文件名，或者周期性的作 Checkpoint，记录成功写入了多少数据。Readers 仅校验并处理上个 Checkpoint 之后产生的文件 region，这些文件 region 的状态一定是已定义的。**追加写入比随机位置写入更加有效率**，对应用程序的失败处理更具有弹性。

在读取数据时，为了避免读入额外的填充数据或是损坏的数据，数据在写入前往往会放入一些如Checksum等信息来验证它的有效性，如此一来客户端便可以自动跳过这些脏数据。如果客户端不能容忍偶尔的重复内容，此时只能由上层应用通过唯一标识符等信息来过滤重复数据了。

# 3 系统交互

在设计这个系统时，**一个重要的原则是最小化所有操作和 Master 节点的交互**。

## 3.1 租约（lease）和变更顺序

使用租约（lease）机制来保持多个副本间变更顺序的一致性。Master为Chunk的一个副本建立一个租约，我们把这个副本叫做主Chunk。主Chunk对所有变更操作进行串行排序（ serial order）。所有的副本都遵从这个排序后的序列进行修改操作。因此，修改操作的全局顺序首先由Master节点选择的租约的顺序决定，然后由租约中的主Chunk分配的序列号决定。

设计租约机制的目的是为了最小化 Master 节点的管理负担。租约的初始超时设置为 60 秒。租约在Chunk被修改的情况下可以申请更长的租期。

![lease](/images/posts/gfs/lease.png)

如图2所示，依据步骤编号来说明一下写入操作的控制流程。

1. 客户机向 Master 节点询问哪一个 Chunk 服务器持有当前的租约，以及其它副本的位置。如果没有一个Chunk 持有租约，Master 节点就选择其中一个副本建立一个租约（这个步骤在图上没有显示）。
2. Master 节点将主 Chunk 的标识符以及其它副本（又称为 secondary 副本、二级副本）的位置返回给客户机。客户机缓存这些数据以便后续的操作。只有在主 Chunk 不可用，或者主 Chunk 回复信息表明它已不再持有租约的时候，客户机才需要重新跟 Master 节点联系。
3. 客户机把数据推送到所有的副本上。客户机可以以任意的顺序推送数据。Chunk 服务器接收到数据并保存在它的内部 LRU 缓存中，一直到数据被使用或者过期交换出去。
4. 当所有的副本都确认接收到了数据，客户机发送写请求到主 Chunk 服务器。这个请求标识了早前推送到所有副本的数据。主 Chunk 为接收到的所有操作分配连续的序列号，这些操作可能来自不同的客户机，序列号保证了操作顺序执行。它以序列号的顺序把操作应用到它自己的本地状态中。
5. 主 Chunk 把写请求传递到所有的二级副本。每个二级副本依照主 Chunk 分配的序列号以相同的顺序执行这些操作。
6. 所有的二级副本回复主 Chunk，它们已经完成了操作。
7. 主 Chunk 服务器回复客户机。任何副本产生的任何错误都会返回给客户机。在出现错误的情况下，写入操作可能在主 Chunk 和一些二级副本执行成功。（如果操作在主 Chunk 上失败了，操作就不会被分配序列号，也不会被传递。）客户端的请求被确认为失败，被修改的 region 处于不一致的状态。我们的客户机代码通过重复执行失败操作来处理这样的错误。在从头开始重复执行之前，客户机会先从步骤3到步骤7做几次尝试。

如果应用程序一次写入的数据量很大，或者数据跨越了多个 Chunk，GFS 客户机代码会把它们分成多个写操作。这些操作都遵循前面描述的控制流程，但是可能会被其它客户机上同时进行的操作打断或者覆盖。

## 3.2 数据流

为了提高网络效率，采取了把数据流和控制流分开的措施。

为了充分利用每台机器的带宽，数据沿着一个 Chunk 服务器链顺序的推送。

为了尽可能的避免出现网络瓶颈和高延迟的链接（eg，inter-switch 最有可能出现类似问题），每台机器都尽量的在网络拓扑中选择一台还没有接收到数据的、离自己最近的机器作为目标推送数据。

最后，利用基于 TCP 连接的、管道式数据推送方式来最小化延迟。

## 3.3 原子的记录追加

GFS 提供了一种原子的数据追加操作–记录追加。GFS 保证至少有一次原子的写入操作成功执行（即写入一个顺序的 byte 流），写入的数据追加到 GFS 指定的偏移位置上，之后 GFS 返回这个偏移量给客户机。

记录追加是一种修改操作，客户机把数据推送给文件最后一个 Chunk 的所有副本，之后发送请求给主 Chunk。主 Chunk 会检查这次记录追加操作是否会使 Chunk 超过最大尺寸（64MB）。如果超过了最大尺寸，主 Chunk 首先将当前 Chunk 填充到最大尺寸，之后通知所有二级副本做同样的操作，然后回复客户机要求其对下一个 Chunk 重新进行记录追加操作。主 Chunk 把数据追加到自己的副本内，然后通知二级副本把数据写在跟主 Chunk 一样的位置上，最后回复客户机操作成功。

如果记录追加操作在任何一个副本上失败了，客户端就需要重新进行操作。这里可能存在几个问题，同一个Chunk的不同副本可能包含不同的数据（重复包含一个记录全部或部分的数据）。**GFS不保证Chunk的所有副本在字节级别是完全一致的**。**只保证数据作为一个整体原子的被至少写入一次**。

## 3.4 快照

快照操作几乎可以瞬间完成对一个文件或者目录树（“源”）做一个拷贝，并且几乎不会对正在进行的其它操作造成任何干扰。GFS用标准的 copy-on-write技术实现快照。

当 Master 节点收到一个快照请求，它首先取消作快照的文件的所有 Chunk 的租约。

租约取消或者过期之后，Master 节点把这个操作以日志的方式记录到硬盘上。

Master 节点通过复制源文件或者目录的元数据的方式，把这条日志记录的变化反映到保存在内存的状态中。新创建的快照文件和源文件指向完全相同的 Chunk 地址。

# 4 Master节点的操作

Master 节点执行所有的名称空间操作。此外，它还管理着整个系统里所有 Chunk 的副本。

1. 决定了Chunk的存储位置，创建新Chunk和它的副本。
2. 协调各种系统活动保证Chunk被完全复制。
3. 在所有的Chunk Server之间进行负载均衡，回收不再使用的存储空间。

## 4.1 名称空间管理和锁

在逻辑上，GFS 的名称空间就是一个全路径和元数据映射关系的查找表。利用前缀压缩，这个表可以高效的存储在内存中。在存储名称空间的树型结构上，每个节点（文件名或目录名）都有一个关联的读写锁。

> 读写之间是互斥的—–>读的时候写阻塞，写的时候读阻塞，而且读和写在竞争锁的时候，写会优先得到锁

演示一下在/home/user 被快照到/save/user 的时候，锁机制如何防止创建文件/home/user/foo。

1. 快照操作获取/home 和/save 的读取锁，以及/home/user 和/save/user 的写入锁。
2. 文件创建操作获得/home 和/home/user 的读取锁，以及/home/user/foo 的写入锁。

这两个操作要顺序执行，因为它们试图获取的/home/user的锁是相互冲突。

采用这种锁方案的优点是支持对同一目录的并行操作。比如，可以再同一个目录下同时创建多个文件：每一个操作都获取一个目录名的上的读取锁和文件名上的写入锁。目录名的读取锁足以的防止目录被删除、改名以及被快照。文件名的写入锁序列化文件创建操作，确保不会多次创建同名的文件。

## 4.2 副本的位置

GFS 集群是高度分布的多层布局结构，而不是平面结构。Chunk Server被来自同一或者不同机架上的数百个客户机轮流访问。不同机架上的两台机器间的通讯可能跨越一个或多个网络交换机。

Chunk 副本位置选择的策略服务两大目标：最大化数据可靠性和可用性，最大化网络带宽利用率。做法是在多个机架间分布储存Chunk的副本。这保证Chunk的一些副本在整个机架被破坏或掉线（比如，共享资源，如电源或者网络交换机造成的问题）的情况下依然存在且保持可用状态。这还意味着在网络流量方面，尤其是针对 Chunk 的读操作，能够有效利用多个机架的整合带宽。

## 4.3 创建，重新复制，重新负载均衡

Chunk 的副本有三个用途：Chunk 创建，重新复制和重新负载均衡。

当 Master 节点创建一个 Chunk 时，它会选择在哪里放置初始的空的副本。Master 节点会考虑几个因素。

1. 在低于平均硬盘使用率的 Chunk 服务器上存储新的副本。
2. 限制在每个 Chunk 服务器上“最近”的 Chunk 创建操作的次数。
3. 把 Chunk 的副本分布在多个机架之间。

具体的，当 Chunk 的有效副本数量少于用户指定的复制因数的时候，Master 节点会重新复制它。

Master 节点选择优先级最高的 Chunk，然后命令某个 Chunk 服务器直接从可用的副本”克隆”一个副本出来。

最后，Master 服务器周期性地对副本进行重新负载均衡：它检查当前的副本分布情况，然后移动副本以便更好的利用硬盘空间、更有效的进行负载均衡。

**这里，似乎可以看到一点Raft的影子了。**

## 4.4 垃圾回收

GFS 在文件删除后不会立刻回收可用的物理空间。GFS 空间回收采用惰性的策略，只在文件和 Chunk 级的常规垃圾收集时进行。

### 4.4.1 机制

当一个文件被应用程序删除时，Master 节点象对待其它修改操作一样，立刻把删除操作以日志的方式记录下来。但是，Master 节点并不马上回收资源，而是把文件名改为一个包含删除时间戳的、隐藏的名字。当Master 节点对文件系统命名空间做常规扫描的时候，它会删除所有三天前的隐藏文件（这个时间间隔是可以设置的）。直到文件被真正删除，它们仍旧可以用新的特殊的名字读取，也可以通过把隐藏文件改名为正常显示的文件名的方式“反删除”。当隐藏文件被从名称空间中删除，Master 服务器内存中保存的这个文件的相关元数据才会被删除。这也有效的切断了文件和它包含的所有 Chunk 的连接。

### 4.4.2 讨论

垃圾回收在空间回收方面相比直接删除有几个优势。

1. 首先，对于组件失效是常态的大规模分布式系统，垃圾回收方式简单可靠。
2. 第二，垃圾回收把存储空间的回收操作合并到 Master 节点规律性的后台活动中， 垃圾回收在 Master 节点相对空闲的时候完成。
3. 延缓存储空间回收为意外的、不可逆转的删除操作提供了安全保障。

## 4.5 过期失效的副本检测

当 Chunk 服务器失效时，Chunk 的副本有可能因错失了一些修改操作而过期失效。Master 节点保存了每个 Chunk 的版本号，用来区分当前的副本和过期副本。

无论何时，只要 Master 节点和 Chunk 签订一个新的租约，它就增加 Chunk 的版本号，然后通知最新的副本。如果某个副本所在的 Chunk 服务器正好处于失效状态，那么副本的版本号就不会被增加。Master 节点在这个 Chunk 服务器重新启动，并且向 Master 节点报告它拥有的 Chunk 的集合以及相应的版本号的时候，就会检测出它包含过期的 Chunk。如果 Master 节点看到一个比它记录的版本号更高的版本号，Master 节点会认为它和 Chunk 服务器签订租约的操作失败了，因此会选择更高的版本号作为当前的版本号。

Master 节点在例行的垃圾回收过程中移除所有的过期失效副本。

# 5 容错和诊断

在设计 GFS 时遇到的最大挑战之一是如何处理频繁发生的组件失效。

## 5.1 高可用性

在 GFS 集群的数百个服务器之中，在任何给定的时间必定会有些服务器是不可用的。我们使用两条简单但是有效的策略保证整个系统的高可用性：**快速恢复和复制**。

### 5.1.1 快速恢复

不管 Master 服务器和 Chunk 服务器是如何关闭的，它们都被设计为可以在数秒钟内恢复它们的状态并重新启动。

### 5.1.2 Chunk复制

每个 Chunk 都被复制到不同机架上的不同的 Chunk 服务器上。用户可以为文件命名空间的不同部分设定不同的复制级别。默认是3。

### 5.1.3 Master服务器的复制

为了保证 Master 服务器的可靠性，Master 服务器的状态也要复制。Master 服务器所有的操作日志和checkpoint 文件都被复制到多台机器上。对 Master 服务器状态的修改操作能够提交成功的前提是，操作日志写入到 Master 服务器的备节点和本机的磁盘。

此外，GFS 中还有些“影子”Master 服务器，这些“影子”服务器在“主”Master 服务器宕机的时候提供文件系统的只读访问。它们是影子，而不是镜像，所以它们的数据可能比“主”Master 服务器更新要慢，通常是不到 1 秒。对于那些不经常改变的文件、或者那些允许获取的数据有少量过期的应用程序，“影子”Master 服务器能够提高读取的效率。

“影子”Master 服务器为了保持自身状态是最新的，它会读取一份当前正在进行的操作的日志副本，并且依照和主 Master 服务器完全相同的顺序来更改内部的数据结构。

> 论文的第6，7，8小节为GFS一些基准测试和经验分享，这里不在赘述。

以上。

---

最后分享一下《纽约客》为两位谷歌传奇工程师Jeff Dean和 Sanjay Ghemawat（也是GFS的一作）撰写的故事。Jeff Dean 和 Sanjay Ghemawat 是仅有的两位达到 Google Senior Fellow 级别的传奇工程师，而且他们二人还是相交甚厚的好朋友。

[The Friendship That Made Google Huge | The New Yorker](https://www.newyorker.com/magazine/2018/12/10/the-friendship-that-made-google-huge?currentPage=all)

