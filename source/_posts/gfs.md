---
title: 关于Google File System（GFS）的一些思考
date: 2022/10/16
tag: Paper
author: Slide
categories: Paper
---

最近在刷MIT6.824，顺便读完了早年谷歌分布式领域非常经典的三大论文之一GFS，发现论文中很多简洁和经典的设计依旧能在20年后的系统中看到影子。GFS文件系统展示了一个使用普通硬件支持大规模数据处理的系统的特质。本文会花一到两周的时间对整篇论文以及工作中遇到的一些问题进行总结和思考。

<!--more-->

# 1. 简介

GFS与传统的分布式文件系统有着很多相同的设计目标。包括性能、可伸缩性、可靠性以及可用性。同时GFS和早期文件系统的假设都有明显的不同，从而衍生出了完全不同的设计思路。

1. 首先组件失效被认为是常态事件，而不是意外事件。这意味着，持续的监控告警，灾难冗余和自动恢复等机制就必须集成在GFS中。
2. 其次，文件非常巨大，I/O操作和块的尺寸都需要重新考虑。
3. 绝大部分的文件的修改是采用在文件尾部追加数据，而不是覆盖原有数据的方式。
4. 应用程序和文件系统API的协同设计提高了整个系统的灵活性。

# 2. 设计概述

## 2.1 设计预期

1. 系统必须持续监控自身的状态，它必须将组件失效作为一种常态，能够迅速地侦测、冗余并恢复失效的组件。
2. 系统的工作负载主要由两种读操作组成：**大规模的流式读取和小规模的随机读取**。
3. 系统的工作负载还包括许多大规模的、顺序的、数据追加方式的写操作。
4. 系统必须高效的、行为定义明确的2实现多客户端并行追加数据到同一个文件里的语意。
5. 高性能的稳定网络带宽远比低延迟重要。

## 2.2 接口

GFS 提供了一套类似传统文件系统的 API 接口函数，虽然并不是严格按照 POSIX 等标准 API 的形式实现的。文件以分层目录的形式组织，用路径名来标识。我们支持常用的操作，如创建新文件、删除文件、打开文件、关闭文件、读和写文件。

## 2.3 架构

![gfs-arch](/images/posts/gfs/gfs-arch.png)

一个 GFS 集群包含一个单独的 Master 节点3、多台 Chunk 服务器，并且同时被多个客户端访问。所有的这些机器通常都是普通的 Linux 机器，运行着用户级别(user-level)的服务进程。

**GFS 存储的文件都被分割成固定大小的 Chunk。**

1. 在 Chunk 创建的时候，Master 服务器会给每个 Chunk 分配一个不变的、全球唯一的 64 位的 Chunk 标识。
2. Chunk 服务器把 Chunk 以 Linux 文件的形式保存在本地硬盘上，并且根据指定的 Chunk 标识和字节范围来读写块数据。每个块都会复制到多个块服务器上。
3. 缺省情况下，使用 3 个存储复制节点，不过用户可以为不同的文件命名空间设定不同的复制级别。

**Master 节点管理所有的文件系统元数据。**

1. 这些元数据包括名字空间、访问控制信息、文件和 Chunk 的映射信息、以及当前 Chunk 的位置信息。
2. 同时管理着系统范围内的活动，比如，租户，孤儿Chunk回收及Chunk迁移。
3. 最后Master会和每个Chunk Server周期性的心跳通讯。

**GFS 客户端代码以库的形式被链接到客户程序里。**

值得注意的是，客户端和Master节点通信仅仅只获取了元数据，所有的数据操作都是由客户端直接和Chunk Server进行交互的。

**无论是客户端还是 Chunk 服务器都不需要缓存文件数据。**

1. 客户端大部分以流的方式读取一个巨大文件，工作集太大无法被缓存。
2. 服务端Chunk以本地文件的方式保存，Linux操作系统的文件系统缓存会把经常访问的数据缓存在内存中。

## 2.4 单一Master节点

在论文中提到，谷歌的工程师们认为，单一的 Master 节点的策略大大简化了设计。单一的 Master 节点可以通过全局的信息精确定位Chunk 的位置以及进行复制决策。基于此，客户端必须减少对Master节点的读写。同时，客户端把Master作为一个类似Nameserver服务来保持跟自己的通信。

具体通信的流程如图1所示。

1. 客户端把文件名和程序指定的字节偏移，根据固定的 Chunk 大小，转换成文件的Chunk 索引。
2. 客户端把文件名和 Chunk 索引发送给 Master 节点。
3. Master 节点将相应的 Chunk 标识和副本的位置信息发还给客户端。
4. 客户端用文件名和 Chunk 索引作为 key 缓存这些信息。
5. 客户端发送请求到其中的一个副本处，一般会选择最近的。请求信息包含了 Chunk 的标识和字节范围。在对这个 Chunk 的后续读取操作中，客户端不必再和 Master 节点通讯了。

## 2.5 Chunk尺寸

Chunk 的大小是关键的设计参数之一。GFS选择了64MB。每个 Chunk 的副本都以普通 Linux 文件的形式保存在 Chunk 服务器上，只有在需要的时候才扩大。惰性空间分配策略避免了因内部碎片造成的空间浪费，内部碎片或许是对选择这么大的 Chunk 尺寸最具争议一点。

选择较大的 Chunk 尺寸有几个重要的优点。

1. 减少了客户端和Master节点通讯的需求。因为只需要一次和Master节点通信就可以获取Chunk的位置信息，之后就可以对同一个Chunk进行多次的读写操作。
2. 客户端能够对一个块进行多次操作，这样就可以通过与 Chunk 服务器保持较长时间的 TCP 连接来减少网络负载。
3. 减少了 Master节点需要保存的元数据的数量。注意，这里Master的元数据是保存在内存中。

同时，较大尺寸的Chunk也是有缺陷的。

小文件可能包含较少的Chunk或者只有一个Chunk，这样当客户端频繁多次访问这个小文件时，该Chunk的Chunk Server容易变成一个热点。















待更新..